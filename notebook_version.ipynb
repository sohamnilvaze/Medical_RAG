{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bcbe6eb",
   "metadata": {},
   "source": [
    "This is the notebook version of running the Gradio interface of Medical RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf050d9",
   "metadata": {},
   "source": [
    "Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6d121",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain spacy nltk faiss-cpu cohere python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126676d4",
   "metadata": {},
   "source": [
    "Loading the API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18c8d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access API key\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30b5f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SpacyTextSplitter, NLTKTextSplitter\n",
    "import nltk\n",
    "import torch\n",
    "import gradio as gr\n",
    "import cohere\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59799262",
   "metadata": {},
   "source": [
    "All chunking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e06d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#semantic chunking\n",
    "def semantic_chunking(text, similarity_threshold):\n",
    "    \"\"\"\n",
    "    Perform semantic chunking on a given text by grouping semantically similar sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to chunk.\n",
    "    - model (SentenceTransformer): Preloaded SentenceTransformer model for embeddings.\n",
    "    - similarity_threshold (float): Threshold to determine if sentences belong in the same chunk.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of chunks, each containing semantically similar sentences.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences\n",
    "    print(\"Splitting the sentences\")\n",
    "    # sentence_pattern = r\"(.*?[.!?])\"\n",
    "    sentence_pattern = r'([^.!?]*[.!?])'\n",
    "    sentences = re.findall(sentence_pattern, text)\n",
    "\n",
    "    # Generate embeddings for sentences\n",
    "    print(\"Creating the embeddings\")\n",
    "    embeddings = embedding_model.encode(sentences)\n",
    "    embeddings_list = embeddings.tolist()\n",
    "\n",
    "    semantic_chunks = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        if i == 0:\n",
    "            semantic_chunks.append(sentences[i])\n",
    "        else:\n",
    "            # Reshape embeddings to 2D arrays for cosine similarity\n",
    "            embedding1 = np.array(embeddings_list[i - 1]).reshape(1, -1)\n",
    "            embedding2 = np.array(embeddings_list[i]).reshape(1, -1)\n",
    "            similarity = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "            if similarity[0][0] >= similarity_threshold:\n",
    "                # Combine the current sentence with the previous chunk\n",
    "                semantic_chunks[-1] += \" \" + sentences[i]\n",
    "            else:\n",
    "                # Start a new chunk\n",
    "                semantic_chunks.append(sentences[i])\n",
    "\n",
    "    return semantic_chunks\n",
    "\n",
    "#Section based chunking\n",
    "headers = [\n",
    "    \"Patient ID\",\n",
    "    \"Admission Date\",\n",
    "    \"Discharge Date\",\n",
    "    \"Date of Birth\",\n",
    "    \"Sex\",\n",
    "    \"Service\",\n",
    "    \"Chief Complaint\",\n",
    "    \"History of Present Illness\",\n",
    "    \"Past Medical History\",\n",
    "    \"Past Surgical History\",\n",
    "    \"Social History\",\n",
    "    \"Family History\",\n",
    "    \"Allergies\",\n",
    "    \"Medications on Admission\",\n",
    "    \"Hospital Course\",\n",
    "    \"Investigations\",\n",
    "    \"Procedures\",\n",
    "    \"Discharge Plan\"\n",
    "]\n",
    "\n",
    "\n",
    "def extract_sections_chunks(text):\n",
    "    # Sort headers by length (descending) to avoid partial matches\n",
    "    chunks=[]\n",
    "    headers = sorted(headers, key=len, reverse=True)\n",
    "\n",
    "    # Create a regex pattern to detect headers\n",
    "    pattern = r\"(?:\" + \"|\".join(re.escape(header) for header in headers) + r\")\"\n",
    "\n",
    "    # Use regex to find all occurrences of headers\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "\n",
    "    section_dict = {}\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        header = match.group().strip()\n",
    "        start_idx = match.end()  # Start of the section content\n",
    "\n",
    "        # Determine end index (either next header or end of text)\n",
    "        end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        content = text[start_idx:end_idx].strip()\n",
    "        section_dict[header] = content\n",
    "        chunks.append(content)\n",
    "    return chunks\n",
    "\n",
    "#Recursive chunking\n",
    "def recursive_chunking(text):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100\n",
    "  )\n",
    "  docs = text_splitter.create_documents([text])\n",
    "  required_notes_chunks4=[]\n",
    "  for doc in docs:\n",
    "    required_notes_chunks4.append(doc.page_content)\n",
    "\n",
    "  return required_notes_chunks4\n",
    "\n",
    "#Fixed size chunking\n",
    "def chunk_string_with_overlap(string):\n",
    "    chunks = []\n",
    "    chunk_size = len(string) // 10\n",
    "    overlap_size = chunk_size // 10\n",
    "    i = 0\n",
    "    while i < len(string) - chunk_size + 1:\n",
    "        chunk = string[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap_size\n",
    "    # Add the last chunk if there's remaining string\n",
    "    if i < len(string):\n",
    "        chunks.append(string[i:])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "#Context aware spacy chunking\n",
    "def context_aware_chunking_spacy(text):\n",
    "  text_splitter =SpacyTextSplitter(chunk_size=400)\n",
    "  docs = text_splitter.split_text(text)\n",
    "  doc_final=[]\n",
    "  for i, doc in enumerate(docs):\n",
    "    doc_final.append(doc)\n",
    "\n",
    "  return doc_final\n",
    "\n",
    "#Context aware nltk chunking\n",
    "def context_aware_chunking_nltk(text):\n",
    "  text_splitter = NLTKTextSplitter(chunk_size=400)\n",
    "  docs = text_splitter.split_text(text)\n",
    "  doc_final=[]\n",
    "  for i, doc in enumerate(docs):\n",
    "    doc_final.append(doc)\n",
    "\n",
    "  return doc_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ccddc",
   "metadata": {},
   "source": [
    "Initializing the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5ce2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "    return embedding_model\n",
    "\n",
    "embedding_model = prepare_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11be8e",
   "metadata": {},
   "source": [
    "Data Ingestion and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a3aba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_id_query(query:str):\n",
    "  match = re.search(r\"(?:patient\\s*id|id)\\s*[:=]?\\s*(\\d+)\", query, re.IGNORECASE)\n",
    "  if match:\n",
    "    return match.group(1)\n",
    "\n",
    "def extract_id(note:str):\n",
    "  match = re.search(r\"Patient ID:\\s*(\\d+)\", note)\n",
    "  if match:\n",
    "    return match.group(1)\n",
    "\n",
    "def read_all_txt_files(folder_path:str,chunk_choice:int) -> list[str]:\n",
    "    contents = []\n",
    "    ids = []\n",
    "\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    id = extract_id(content)\n",
    "                    chunks = []\n",
    "                    if chunk_choice == 0:\n",
    "                        chunks = chunk_string_with_overlap(content)\n",
    "                    elif chunk_choice == 1:\n",
    "                        chunks = semantic_chunking(content,0.50)\n",
    "                    elif chunk_choice == 2:\n",
    "                        chunks = extract_sections_chunks(content)\n",
    "                    elif chunk_choice == 3:\n",
    "                        chunks = recursive_chunking(content)\n",
    "                    elif chunk_choice == 4:\n",
    "                        chunks = context_aware_chunking_spacy(content)\n",
    "                    else:\n",
    "                        chunks = context_aware_chunking_nltk(content)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    contents.append(chunk)\n",
    "                    ids.append(id)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Invalid folder path: {folder_path}\")\n",
    "\n",
    "    return contents, ids\n",
    "\n",
    "\n",
    "def get_embeddings(embedding_model,text):\n",
    "    # embedding_model = prepare_model()\n",
    "    embeddings = embedding_model.encode(text,convert_to_numpy=True)\n",
    "    return embeddings.tolist()\n",
    "\n",
    "def prepare_vector_database(embedding_model,contents : list[str], ids):\n",
    "    df = pd.DataFrame({'Document content':contents,\"Patient ID\":ids})\n",
    "    df[\"Embeddings\"] = df['Document content'].apply(lambda x:get_embeddings(embedding_model,text = x))\n",
    "\n",
    "    index = faiss.IndexFlatL2(384)\n",
    "\n",
    "    metadata_store = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        emb = np.array(df['Embeddings'][i]).astype('float32')\n",
    "        emb = emb.reshape(1,-1)\n",
    "        index.add(emb)\n",
    "\n",
    "        metadata_store[i] = {\n",
    "            \"patient_id\": df['Patient ID'][i],\n",
    "            \"note\": df['Document content'][i]\n",
    "        }\n",
    "\n",
    "    return index, df, metadata_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154220a",
   "metadata": {},
   "source": [
    "Context preparation and Query handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5524fb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_content(query:str, index, df,embedding_model,k = 3):\n",
    "    query_id = extract_id_query(query)\n",
    "    query_embedding = get_embeddings(embedding_model,query)\n",
    "    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n",
    "    D, I = index.search(query_embedding,k = k)\n",
    "    retrieved_content = \"\"\n",
    "    tot_cis_score = 0.0\n",
    "    retrieved_count = 0\n",
    "    for doc_index, distance in zip(I[0], D[0]):\n",
    "        if doc_index < 0 or doc_index>=len(df):\n",
    "            print(f\"Invalid index: {doc_index}, dataframe length = {len(df)}\")\n",
    "            continue\n",
    "        # else:\n",
    "        #     document = df.iloc[doc_index][\"Document content\"]\n",
    "        #     retrieved_content = retrieved_content + \" \" + document\n",
    "        #     cosine_score = 1 - distance / 2\n",
    "        #     tot_cis_score = tot_cis_score + cosine_score\n",
    "        patient_id = df.iloc[doc_index][\"Patient ID\"]\n",
    "        if str(patient_id) != str(query_id):\n",
    "          continue\n",
    "\n",
    "        document = df.iloc[doc_index][\"Document content\"]\n",
    "        retrieved_content = retrieved_content + document\n",
    "        cosine_score = 1 - distance / 2\n",
    "        tot_cis_score = tot_cis_score + cosine_score\n",
    "        retrieved_count = retrieved_count + 1\n",
    "        if retrieved_count>=k:\n",
    "          break\n",
    "\n",
    "    if retrieved_content.strip() == \"\":\n",
    "        retrieved_content = \"No notes found for this patient ID.\"\n",
    "\n",
    "    return retrieved_content, tot_cis_score\n",
    "\n",
    "\n",
    "def answer_with_content(query:str, index, df,embedding_model):\n",
    "\n",
    "    co = cohere.ClientV2(cohere_api_key)\n",
    "    retrieved_content, tot_score = prepare_content(query,index, df,embedding_model)\n",
    "    additional_context = \"\"\"\n",
    "      You are an expert medical assistant tasked with answering questions based on the provided clinical context.\n",
    "      The context contains important sections or headers. Please ensure that you pay close attention to these headers\n",
    "      and use the most relevant section to answer the query. Your response should be clear, concise, medically accurate,\n",
    "      and tailored to the specific medical scenario. Always provide a full-sentence answer to the query based on the\n",
    "      information from the most appropriate section of the context.\n",
    "\n",
    "      Context: {context}\n",
    "\n",
    "      Question: {query}\n",
    "\n",
    "      Answer:\n",
    "      \"\"\"\n",
    "    combined_prompt = additional_context.format(context=retrieved_content, query=query)\n",
    "\n",
    "\n",
    "    response = co.chat(\n",
    "        model = 'command-xlarge-nightly',\n",
    "         messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": combined_prompt,\n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    print(response.message.content[0].text)\n",
    "    print(type(response.message.content))\n",
    "    print(len(response.message.content))\n",
    "    response1 = response.message.content[0].text\n",
    "\n",
    "    return response1,tot_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d2cba",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e6fe6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Reading all text files\")\n",
    "all_contents, ids = read_all_txt_files(\"/content/sample_data\",1)\n",
    "\n",
    "print(\"Preparing Faiss vector database\")\n",
    "faiss_index, content_df, metadata = prepare_vector_database(embedding_model,all_contents,ids)\n",
    "\n",
    "def process_query(query:str):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a query\"\n",
    "\n",
    "    required_content, score = prepare_content(query,faiss_index,content_df,embedding_model)\n",
    "    print(required_content)\n",
    "    print(score)\n",
    "    answer, score = answer_with_content(query,faiss_index,content_df,embedding_model)\n",
    "\n",
    "    return answer, f\"Confidence Score: {score}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2a9dd",
   "metadata": {},
   "source": [
    "Gradio  interface launching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d89dc",
   "metadata": {},
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Medical RAG system\")\n",
    "    gr.Markdown(\"Ask query and get answer based on the notes provided.\")\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(label=\"Enter your query\", placeholder=\"Type your question here...\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Get Answer\")\n",
    "\n",
    "    with gr.Row():\n",
    "        answer_output = gr.Textbox(label=\"Answer\", lines=5)\n",
    "\n",
    "    with gr.Row():\n",
    "        score_output = gr.Number(label=\"Score\")\n",
    "\n",
    "    submit_btn.click(fn=process_query, inputs=query_input, outputs=[answer_output, score_output])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug = True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
